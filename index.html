<html>

  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/black.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
  </head>

  <body>
    <div class="reveal">
      <div class="slides">

	<section data-background="img/crs4.png" data-background-size="331px" data-background-position="center 98%">
	  <h3>Integrating Python with other languages: the Pydoop case</h3>
	  <p><small>Simone Leo &ndash; CRS4</small></p>
	</section>

	<section>
	  <h3>These slides</h3>
	  <ul>
	    <li>(browse) <a href="https://crs4.github.io/pydoop-pycon-sard" target="_blank">crs4.github.io/pydoop-pycon-sard</a></li>
	    <li>(src) <a href="https://github.com/crs4/pydoop-pycon-sard" target="_blank">github.com/crs4/pydoop-pycon-sard</a></li>
	  </ul>
	</section>

	<section>
	  <div style="text-align:left">
	    <dl>
	      <dt>Q.</dt>
	      <dd>What is <a href="https://crs4.github.io/pydoop" target="_blank">Pydoop</a>?</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>A.</dt>
	      <dd>A set of Python bindings for Hadoop</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>Q.</dt>
	      <dd>OK, but what is <a href="http://hadoop.apache.org/" target="_blank">Hadoop</a>?</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>A.</dt>
	      <dd>A Java distributed computing (DC) framework</dd>
	    </dl>
	  </div>
	</section>

	<section>  <!-- START HADOOP -->
	  <section>
	    <h3>Hadoop</h3>
	    <ul>
	      <li>DC framework focused on data-intensive jobs</li>
	      <li class="fragment">Simple programming model (MapReduce)</li>
	      <li class="fragment">Backed up by a distributed filesystem (HDFS)</li>
	      <li class="fragment">Written in Java</li>
	    </ul>
	  </section>
	  <section>
	    <h3>MapReduce</h3>
	    <object type="image/svg+xml" data="img/mapreduce.svg">
	      SVG not supported
	    </object>
	  </section>
	  <section>
	    <h3>Word count &ndash; pseudocode</h3>
	    <pre><code class="hljs" data-trim>
void map(Object key, String value) {
  for (String word: tokenize(value)) {
    emit(word, 1);
  }
}

void reduce(String key, Iterator values) {
  int count = 0;
  for (int v: values) {
    count += v;
  }
  emit(key, count);
}
	    </code></pre>
	  </section>
	  <section>
	    <h3>Word count &ndash; Java</h3>
	    <pre><code class="java" data-trim>
public static class TokenizerMapper
    extends Mapper&lt;Object, Text, Text, IntWritable&gt; {
  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();
  public void map(Object k, Text v, Context context)
      throws IOException, InterruptedException {
    StringTokenizer itr = new StringTokenizer(v.toString());
    while (itr.hasMoreTokens()) {
      word.set(itr.nextToken());
      context.write(word, one);
    }
  }
}

public static class IntSumReducer
     extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
  private IntWritable result = new IntWritable();
  public void reduce(Text k, Iterable&lt;IntWritable&gt; values,
      Context context)
      throws IOException, InterruptedException {
    int sum = 0;
    for (IntWritable v: values) {
      sum += v.get();
    }
    result.set(sum);
    context.write(k, result);
  }
}
	    </code></pre>
	  </section>
	</section>  <!-- END HADOOP -->

	<section>
	  <h3>Pydoop</h3>
	  <p>
	    <ul>
	      <li>Python bindings for Hadoop</li>
	      <li>MapReduce & HDFS API</li>
	      <li>Started 10 years ago with the following goal</li>
	    </ul>
	  </p>
	  <pre><code class="python" data-trim>
class Mapper(api.Mapper):
    def map(self, context):
        for w in context.value.split():
            context.emit(w, 1)

class Reducer(api.Reducer):
    def reduce(self, context):
        context.emit(context.key, sum(context.values))
	  </code></pre>
	</section>

	<section>  <!-- START STREAMING & CO. -->
	  <section>
	    <h3>Already available in Hadoop</h3>
	    <p style="text-align:left"><span style="font-weight: bold">MapReduce:</span></p>
	    <div style="text-align:left">
	      <ul class="fragment">
		<li><a href="https://hadoop.apache.org/docs/r3.1.1/hadoop-streaming/HadoopStreaming.html" target="_blank">Hadoop Streaming</a></li>
		<ul>
		  <li>any language (write executable scripts)</li>
		  <li>exchange k/v pairs via stdin/stdout</li>
		</ul>
	      </ul>
	    </div>
	    <div style="text-align:left">
	      <ul class="fragment">
		<li><a href="https://hadoop.apache.org/docs/r1.2.1/api/org/apache/hadoop/mapred/pipes/package-summary.html" target="_blank">Hadoop Pipes</a> (no howto in current version!)</li>
		<ul>
		  <li>any language (a C++ client is included)</li>
		  <li>communicate with framework via socket</li>
		</ul>
	      </ul>
	    </div>
	    <p class="fragment" style="text-align:left"><span style="font-weight: bold">HDFS:</span> <a href="https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-hdfs/LibHdfs.html" target="_blank">libhdfs</a> (C, via JNI)</p>
	  </section>
	  <!-- Add a diagram showing the streams you get from the framework? -->
	  <section>
	    <h3>Hadoop Streaming</h3>
	    <p style="text-align:left"><small><code>mapper.py</code></small></p>
	    <pre><code class="python" data-trim>
for line in sys.stdin:
    for word in line.split():
        sys.stdout.write("%s\t1\n" % word)
	    </code></pre>
	    <p style="text-align:left"><small><code>reducer.py</code></small></p>
	    <pre><code class="python" data-trim>
out_k, out_v = None, 0
for line in sys.stdin:
    k, v = line.split("\t", 1)
    v = int(v)
    if k != out_k:
        if out_k is not None:
            sys.stdout.write("%s\t%d\n" % (out_k, out_v))
            out_v = 0
        out_k = k
    out_v += v
sys.stdout.write("%s\t%d\n" % (out_k, out_v))
	    </code></pre>
	  </section>
	  <section>
	    <h3>Hadoop Streaming &ndash; Fancy Reducer</h3>
	    <pre><code class="python" data-trim>
#!/usr/bin/env python

import sys
from itertools import groupby
from operator import itemgetter

def istream():
    for line in sys.stdin:
        k, v = line.split("\t", 1)
        yield k, int(v)

for k, stream in groupby(istream(), itemgetter(0)):
    v = sum(_[1] for _ in stream)
    sys.stdout.write("%s\t%d\n" % (k, v))
	    </code></pre>
	    <p><a href="https://pythonhosted.org/mrjob" target="_blank">mrjob</a> is based on Hadoop Streaming</p>
	  </section>
	  <section>
	    <h3>Hadoop Pipes</h3>
	    <object type="image/svg+xml" data="img/pipes.svg">
	      SVG not supported
	    </object>
	  </section>
	</section>  <!-- END STREAMING & CO. -->

	<section>  <!-- START PYDOOP INTERNALS -->
	  <section>
	    <h3>Python Pipes Task Runner</h3>
	    <p style="text-align:left"><span style="font-weight: bold">Options, options &hellip;</span></p>
	    <div style="text-align:left">
	      <ul class="fragment">
		<li>Wrap the whole C++ client code</li>
		<ul><li>First Pydoop version (via <a href="https://www.boost.org/doc/libs/1_68_0/libs/python/doc/html/index.html" target="_blank">boost.python</a>)</li></ul>
	      </ul>
	      <ul class="fragment">
		<li>Write everything in pure Python</li>
		<ul><li>Second thing we tried, too slow</li></ul>
	      </ul>
	      <ul class="fragment">
		<li>Extension module only for performance-critical stuff</li>
		<ul><li>Current approach, via the <a href="https://docs.python.org/3/c-api/index.html" target="_blank">C API</a></li></ul>
	      </ul>
	    </div>
	  </section>
	  <section>
	    <h3>Main job: read/write Java types</h3>
	    <pre><code class="java" data-trim>
public static void writeVLong(DataOutput stream, long i) {
  if (i &gt;= -112 && i &lt;= 127) {
    stream.writeByte((byte)i);
    return;
  }
  int len = -112;
  if (i &lt; 0) {
    i ^= -1L;
    len = -120;
  }
  long tmp = i;
  while (tmp != 0) {
    tmp = tmp &gt;&gt; 8;
    len--;
  }
  stream.writeByte((byte)len);
  len = (len &lt; -120) ? -(len + 120) : -(len + 112);
  for (int idx = len; idx != 0; idx--) {
    int shiftbits = (idx - 1) * 8;
    long mask = 0xFFL &lt;&lt; shiftbits;
    stream.writeByte((byte)((i & mask) &gt;&gt; shiftbits));
  }
}
	    </code></pre>
	    <p><small><code>org.apache.hadoop.io.WritableUtils</code></small></p>
	  </section>
	  <section>
	    <h3>Read/write Java types &ndash; C++</h3>
	    <pre><code class="c++" data-trim>
int64_t deserializeLong(InStream& stream) {
  int8_t b;
  stream.read(&b, 1);
  if (b >= -112) {
    return b;
  }
  bool negative;
  int len;
  if (b < -120) {
    negative = true;
    len = -120 - b;
  } else {
    negative = false;
    len = -112 - b;
  }
  uint8_t barr[len];
  stream.read(barr, len);
  int64_t t = 0;
  for (int idx = 0; idx < len; idx++) {
    t = t << 8;
    t |= (barr[idx] & 0xFF);
  }
  if (negative) {
    t ^= -1ll;
  }
  return t;
}
	    </code></pre>
	    <p><small><code>HadoopUtils</code></small></p>
	  </section>
	  <section>
	    <h3>Read/write Java types: Python</h3>
	    <pre><code class="c" data-trim>
static PyObject *
FileInStream_readVLong(FileInStreamObj *self) {
  int64_t rval;
  PyThreadState *state = PyEval_SaveThread();
  try {
    rval = HadoopUtils::deserializeLong(*self->stream);
  } catch (HadoopUtils::Error e) {
    PyEval_RestoreThread(state);
    PyErr_SetString(PyExc_IOError, e.getMessage().c_str());
    return NULL;
  }
  PyEval_RestoreThread(state);
  return Py_BuildValue("L", rval);
}
	    </code></pre>
	  </section>
	  <section>
	    <h3>Task runner</h3>
	    <pre><code class="python" data-trim>
import pydoop.sercore as sercore

class BinaryProtocol(object):

    def __init__(self, istream, ...):
        self.istream = istream

    def __next__(self):
        cmd = self.stream.read_vlong()
        if cmd == AUTHENTICATION_REQ:
            digest = self.stream.read_bytes()
            challenge = self.stream.read_bytes()
            self.verify_digest(digest, challenge)
        elif cmd == START: ...
	    </code></pre>
	  </section>
	  <section>
	    <h3>HDFS</h3>
	    <pre><code class="C" data-trim>
tOffset hdfsGetDefaultBlockSize(hdfsFS fs) {
  jobject jFS = (jobject)fs;
  jvalue jVal;
  jthrowable jthr;
  JNIEnv* env = getJNIEnv();
  if (env == NULL) {
    errno = EINTERNAL;
    return -1;
  }
  jthr = invokeMethod(env, &jVal, INSTANCE, jFS, HADOOP_FS,
                      "getDefaultBlockSize", "()J");
  if (jthr) ... /* handle error */
  return jVal.j;
}
	    </code></pre>
	    <pre><code class="C" data-trim>
PyObject* FsClass_get_default_block_size(FsInfo* self) {
  tOffset size = hdfsGetDefaultBlockSize(self->_fs);
  return PyLong_FromSsize_t(size);
}
	    </code></pre>
	  </section>
	</section>  <!-- END PYDOOP INTERNALS -->

	<section>
	  <div style="text-align:left">
	    <dl>
	      <dt>Q.</dt>
	      <dd>Can I wrap Java code without going through JNI?</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>A.</dt>
	      <dd>Yes, we tried <a href="http://jpype.sourceforge.net" target="_blank">JPype</a> at some point. Too slow.</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>Q.</dt>
	      <dd>Any other options for integrating Python with X?</dd>
	    </dl>
	  </div>
	  <div class="fragment" style="text-align:left">
	    <dl>
	      <dt>A.</dt>
	      <dd>Check out <a href="https://wiki.python.org/moin/IntegratingPythonWithOtherLanguages" target="_blank">IntegratingPythonWithOtherLanguages</a> at <a href="https://wiki.python.org" target="_blank">wiki.python.org</a></dd>
	    </dl>
	  </div>
	</section>

	<section>
	  <h3>Thank you</h3>
	</section>

      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      Reveal.initialize({
        history: true,
        dependencies: [
          {src: 'plugin/highlight/highlight.js',
           async: true,
           callback: function() { hljs.initHighlightingOnLoad(); }}
        ]
      });
    </script>
  </body>

</html>
